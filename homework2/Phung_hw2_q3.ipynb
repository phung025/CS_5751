{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Devise three example tasks of your own that fit into the MDP framework, identifying for each its states, actions, and rewards. Make the three examples as different from each other as possible. The framework is abstract and flexible and can be applied in many different ways. Stretch its limits in some way in at least one of your examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1: A helicopter trying to learn to fly by itself"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**States**: The current position of the helicopter in the space such as height from the ground, the speed of the wing, the distance between the helicopter and the other objects\n",
    "\n",
    "**Actions**: Increase and decrease the height, rotate left or right, fly forward or backward, etc.\n",
    "\n",
    "**Rewards**: A numerical value for the action of the helicopter given its current state such as positive for flying to the correct coordinate and give a negative value for crashing the helicopter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2: Teaching an agent to solve a mathematical equation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**States**: The current state of the equation that is being solved\n",
    "\n",
    "**Actions**: Different mathematical techniques on the equation such as adding, removing value, substituting value, factoring value, etc.\n",
    "\n",
    "**Rewards**: A positive numerical value if the agent apply correct technique to the equation. The reward will be higher if the agent can simplify the equation as much as possible. We punish the agent with negative value if the answer is incorrect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 3: An agent learning to play a game of chess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**States**: The current state of the board game (i.e. The current positions of the chess pieces on the board, the number of pieces captured by each player)\n",
    "\n",
    "**Actions**: Move a chess piece belong to the agent to a position that is allowed\n",
    "\n",
    "**Rewards**: A positive numerical value if the agent make a good move (capture a valuable piece, capture the king, make a good opening move, etc). Punish the agent with a negative value if it loses a valuable chess piece, trade a valuable chess piece with a less valuable piece, etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
