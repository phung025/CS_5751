{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write down the Bellman equations for $v_\\pi(s)$ for the states (0,0), (0,1) and (2,1) in this Gridworld, using the policy described above. This means plugging in the right numbers and probabilities, not just writing the abstract formulas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$v_\\pi\\Big((0,0)\\Big) = (0.25)[-1+\\gamma v_\\pi\\Big((0,0)\\Big)\\Big] + (0.25)[-1+\\gamma v_\\pi\\Big((0,0)\\Big)\\Big] + (0.25)[0+\\gamma v_\\pi\\Big((0,1)\\Big)\\Big] + (0.25)[0+\\gamma v_\\pi\\Big((1,0)\\Big)\\Big] $\n",
    "\n",
    "$v_\\pi\\Big((0,0)\\Big) = (0.25)[-1+\\gamma v_\\pi\\Big((0,0)\\Big)\\Big] + (0.25)[-1+\\gamma v_\\pi\\Big((0,0)\\Big)\\Big] + (0.25)[0+\\gamma v_\\pi\\Big((0,1)\\Big)\\Big] + (0.25)[0+\\gamma v_\\pi\\Big((1,0)\\Big)\\Big]$\n",
    "\n",
    "$v_\\pi\\Big((0,0)\\Big) = (0.25)\\Big[-1+\\gamma v_\\pi\\Big((0,0)\\Big) -1+\\gamma v_\\pi\\Big((0,0)\\Big) + 0+\\gamma v_\\pi\\Big((0,1)\\Big) + 0+\\gamma v_\\pi\\Big((1,0)\\Big)\\Big]$\n",
    "\n",
    "$v_\\pi\\Big((0,0)\\Big) = (0.25)\\Big[2\\gamma v_\\pi\\Big((0,0)\\Big) + \\gamma v_\\pi\\Big((0,1)\\Big) + \\gamma v_\\pi\\Big((1,0)\\Big) - 2 \\Big]$\n",
    "\n",
    "$v_\\pi\\Big((0,0)\\Big) = \\frac{1}{2}\\gamma v_\\pi\\Big((0,0)\\Big) + \\frac{1}{4}\\gamma v_\\pi\\Big((0,1)\\Big) + \\frac{1}{4}\\gamma v_\\pi\\Big((1,0)\\Big) - \\frac{1}{2}$\n",
    "\n",
    "$v_\\pi\\Big((0,0)\\Big)\\Big(1-\\gamma\\frac{1}{2}\\Big) = \\frac{1}{4}\\gamma v_\\pi\\Big((0,1)\\Big) + \\frac{1}{4}\\gamma v_\\pi\\Big((1,0)\\Big) - \\frac{1}{2}$\n",
    "\n",
    "$v_\\pi\\Big((0,0)\\Big) = \\frac{\\frac{1}{4}\\gamma v_\\pi\\Big((0,1)\\Big) + \\frac{1}{4}\\gamma v_\\pi\\Big((1,0)\\Big) - \\frac{1}{2}}{\\Big(1-\\gamma\\frac{1}{2}\\Big)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$v_\\pi(0,1) = (0.25)\\Big[ 40+\\gamma v_\\pi(1,4) + \\gamma v_\\pi(1,4) + \\gamma v_\\pi(1,4) + \\gamma v_\\pi(1,4) \\Big]$\n",
    "\n",
    "$v_\\pi(0,1) = 10 + \\frac{\\gamma}{4} 4v_\\pi(1,4)$\n",
    "\n",
    "$v_\\pi(0,1) = 10 + \\gamma v_\\pi(1,4)$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$v_\\pi(2,1) = (0.25)\\Big[ \\gamma v_\\pi(2,0) + \\gamma v_\\pi(2,2) + \\gamma v_\\pi(1,1) + \\gamma v_\\pi(3,1) \\Big]$\n",
    "\n",
    "$v_\\pi(2,1) = \\frac{\\gamma}{4} v_\\pi(2,0) + \\frac{\\gamma}{4} v_\\pi(2,2) + \\frac{\\gamma}{4} v_\\pi(1,1) + \\frac{\\gamma}{4} v_\\pi(3,1) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using a solver for systems of linear equations, solve the complete system of value-state Bellman equations to obtain $v_\\pi(s)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import Series, DataFrame\n",
    "from numpy.linalg import inv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class gridworld_env:\n",
    "    def __init__(self, x, y):\n",
    "        \n",
    "        # The board of the grid world\n",
    "        self.world = np.arange(0,25).reshape((5,5))\n",
    "        \n",
    "        # Current position of the agent on the board\n",
    "        self.current_position = (x,y)\n",
    "        \n",
    "        # Gamma coefficient\n",
    "        self.gamma = 0.9\n",
    "        \n",
    "        # Possible moves\n",
    "        self.WEST = 0\n",
    "        self.NORTH = 1\n",
    "        self.EAST = 2\n",
    "        self.SOUTH = 3\n",
    "        self.action_spaces = [self.WEST, self.NORTH, self.EAST, self.SOUTH]\n",
    "        \n",
    "        # Special states\n",
    "        self.aprime = (0,1)\n",
    "        self.bprime = (0,3)\n",
    "    '''\n",
    "    Agent take an action to move to new state, the environment returns \n",
    "    the reward for the action\n",
    "    '''\n",
    "    def take_action(self, action):\n",
    "        \n",
    "        # Compute the reward and new position of the agent for the action\n",
    "        reward = 0\n",
    "        new_position = self.current_position\n",
    "        if action == self.NORTH:\n",
    "            new_position = (max(self.current_position[0]-1,0), self.current_position[1])\n",
    "            if new_position == self.current_position:\n",
    "                reward = -1\n",
    "        elif action == self.WEST:\n",
    "            new_position = (self.current_position[0], max(self.current_position[1]-1,0))\n",
    "            if new_position == self.current_position:\n",
    "                reward = -1\n",
    "        elif action == self.EAST:\n",
    "            new_position = (self.current_position[0], (self.current_position[1]+1)%5)\n",
    "            if new_position[1] == 0:\n",
    "                reward = -1\n",
    "                new_position = self.current_position\n",
    "        elif action == self.SOUTH:\n",
    "            new_position = ((self.current_position[0]+1)%5, self.current_position[1])\n",
    "            if new_position[0] == 0:\n",
    "                reward = -1\n",
    "                new_position = self.current_position\n",
    "        \n",
    "        # Special reward if the agent is in A prime or B prime, then\n",
    "        # we can ignore the computation of the reward above\n",
    "        if self.current_position == self.aprime:\n",
    "            reward = 10\n",
    "            new_position = (4,1)\n",
    "        elif self.current_position == self.bprime:\n",
    "            reward = 5\n",
    "            new_position = (2,3)\n",
    "            \n",
    "        # Create a new state of the gridworld\n",
    "        new_state = gridworld_env(new_position[0], new_position[1])\n",
    "        \n",
    "        # Return the reward of the action and the new state of the gridworld\n",
    "        return reward, new_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.zeros((25,25))\n",
    "Y = np.zeros((25,1))\n",
    "\n",
    "# Available actions\n",
    "actions_space = [0, 1, 2, 3]\n",
    "\n",
    "# Gamma coefficient\n",
    "gamma = 0.9\n",
    "\n",
    "for y in range(0, 25):\n",
    "    total_reward = 0\n",
    "    for x in range(0, 25):\n",
    "        env = gridworld_env(int(y/5), y%5)\n",
    "        state_counter = 0\n",
    "        s = 0\n",
    "        for action in actions_space:\n",
    "            reward, new_state = env.take_action(action)\n",
    "            if new_state.current_position == (int(x/5), x%5):\n",
    "                state_counter += 1\n",
    "                total_reward += reward\n",
    "                \n",
    "            if env.current_position == (int(x/5), x%5):\n",
    "                s = 1\n",
    "        value = (gamma * state_counter) / 4 - s\n",
    "        X[y][x] = value\n",
    "        Y[y] = -total_reward/4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.55   0.225  0.     0.     0.     0.225  0.     0.     0.     0.     0.\n",
      "   0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "   0.     0.     0.   ]\n",
      " [ 0.    -1.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "   0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.9\n",
      "   0.     0.     0.   ]\n",
      " [ 0.     0.225 -0.775  0.225  0.     0.     0.     0.225  0.     0.     0.\n",
      "   0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "   0.     0.     0.   ]\n",
      " [ 0.     0.     0.    -1.     0.     0.     0.     0.     0.     0.     0.\n",
      "   0.     0.     0.9    0.     0.     0.     0.     0.     0.     0.     0.\n",
      "   0.     0.     0.   ]\n",
      " [ 0.     0.     0.     0.225 -0.55   0.     0.     0.     0.     0.225  0.\n",
      "   0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "   0.     0.     0.   ]\n",
      " [ 0.225  0.     0.     0.     0.    -0.775  0.225  0.     0.     0.     0.225\n",
      "   0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "   0.     0.     0.   ]\n",
      " [ 0.     0.225  0.     0.     0.     0.225 -1.     0.225  0.     0.     0.\n",
      "   0.225  0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "   0.     0.     0.   ]\n",
      " [ 0.     0.     0.225  0.     0.     0.     0.225 -1.     0.225  0.     0.\n",
      "   0.     0.225  0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "   0.     0.     0.   ]\n",
      " [ 0.     0.     0.     0.225  0.     0.     0.     0.225 -1.     0.225  0.\n",
      "   0.     0.     0.225  0.     0.     0.     0.     0.     0.     0.     0.\n",
      "   0.     0.     0.   ]\n",
      " [ 0.     0.     0.     0.     0.225  0.     0.     0.     0.225 -0.775  0.\n",
      "   0.     0.     0.     0.225  0.     0.     0.     0.     0.     0.     0.\n",
      "   0.     0.     0.   ]\n",
      " [ 0.     0.     0.     0.     0.     0.225  0.     0.     0.     0.    -0.775\n",
      "   0.225  0.     0.     0.     0.225  0.     0.     0.     0.     0.     0.\n",
      "   0.     0.     0.   ]\n",
      " [ 0.     0.     0.     0.     0.     0.     0.225  0.     0.     0.     0.225\n",
      "  -1.     0.225  0.     0.     0.     0.225  0.     0.     0.     0.     0.\n",
      "   0.     0.     0.   ]\n",
      " [ 0.     0.     0.     0.     0.     0.     0.     0.225  0.     0.     0.\n",
      "   0.225 -1.     0.225  0.     0.     0.     0.225  0.     0.     0.     0.\n",
      "   0.     0.     0.   ]\n",
      " [ 0.     0.     0.     0.     0.     0.     0.     0.     0.225  0.     0.\n",
      "   0.     0.225 -1.     0.225  0.     0.     0.     0.225  0.     0.     0.\n",
      "   0.     0.     0.   ]\n",
      " [ 0.     0.     0.     0.     0.     0.     0.     0.     0.     0.225  0.\n",
      "   0.     0.     0.225 -0.775  0.     0.     0.     0.     0.225  0.     0.\n",
      "   0.     0.     0.   ]\n",
      " [ 0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.225\n",
      "   0.     0.     0.     0.    -0.775  0.225  0.     0.     0.     0.225  0.\n",
      "   0.     0.     0.   ]\n",
      " [ 0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "   0.225  0.     0.     0.     0.225 -1.     0.225  0.     0.     0.     0.225\n",
      "   0.     0.     0.   ]\n",
      " [ 0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "   0.     0.225  0.     0.     0.     0.225 -1.     0.225  0.     0.     0.\n",
      "   0.225  0.     0.   ]\n",
      " [ 0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "   0.     0.     0.225  0.     0.     0.     0.225 -1.     0.225  0.     0.\n",
      "   0.     0.225  0.   ]\n",
      " [ 0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "   0.     0.     0.     0.225  0.     0.     0.     0.225 -0.775  0.     0.\n",
      "   0.     0.     0.225]\n",
      " [ 0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "   0.     0.     0.     0.     0.225  0.     0.     0.     0.    -0.55\n",
      "   0.225  0.     0.     0.   ]\n",
      " [ 0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "   0.     0.     0.     0.     0.     0.225  0.     0.     0.     0.225\n",
      "  -0.775  0.225  0.     0.   ]\n",
      " [ 0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "   0.     0.     0.     0.     0.     0.     0.225  0.     0.     0.     0.225\n",
      "  -0.775  0.225  0.   ]\n",
      " [ 0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "   0.     0.     0.     0.     0.     0.     0.     0.225  0.     0.     0.\n",
      "   0.225 -0.775  0.225]\n",
      " [ 0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "   0.     0.     0.     0.     0.     0.     0.     0.     0.225  0.     0.\n",
      "   0.     0.225 -0.55 ]]\n"
     ]
    }
   ],
   "source": [
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0.5 ]\n",
      " [-10.  ]\n",
      " [  0.25]\n",
      " [ -5.  ]\n",
      " [  0.5 ]\n",
      " [  0.25]\n",
      " [  0.  ]\n",
      " [  0.  ]\n",
      " [  0.  ]\n",
      " [  0.25]\n",
      " [  0.25]\n",
      " [  0.  ]\n",
      " [  0.  ]\n",
      " [  0.  ]\n",
      " [  0.25]\n",
      " [  0.25]\n",
      " [  0.  ]\n",
      " [  0.  ]\n",
      " [  0.  ]\n",
      " [  0.25]\n",
      " [  0.5 ]\n",
      " [  0.25]\n",
      " [  0.25]\n",
      " [  0.25]\n",
      " [  0.5 ]]\n"
     ]
    }
   ],
   "source": [
    "print(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_pi = np.linalg.solve(X,Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the solution you obtained in part 5b, print an array such that at state s it displays the value $v_\\pi(s)$ , for the policy $\\pi$. In other words, this exercise is asking you to replicate Figure 3.2 in the book."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 3.30899634  8.78929186  4.42761918  5.32236759  1.49217876]\n",
      " [ 1.52158807  2.99231786  2.25013995  1.9075717   0.54740271]\n",
      " [ 0.05082249  0.73817059  0.67311326  0.35818621 -0.40314114]\n",
      " [-0.9735923  -0.43549543 -0.35488227 -0.58560509 -1.18307508]\n",
      " [-1.85770055 -1.34523126 -1.22926726 -1.42291815 -1.97517905]]\n"
     ]
    }
   ],
   "source": [
    "print(v_pi.reshape((5,5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
