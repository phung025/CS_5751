{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import Series, DataFrame\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class gridworld_env:\n",
    "    def __init__(self, x, y):\n",
    "        \n",
    "        # The board of the grid world\n",
    "        self.world = np.arange(0,25).reshape((5,5))\n",
    "        \n",
    "        # Current position of the agent on the board\n",
    "        self.current_position = (x,y)\n",
    "        \n",
    "        # Gamma coefficient\n",
    "        self.gamma = 0.9\n",
    "        \n",
    "        # Possible moves\n",
    "        self.WEST = 0\n",
    "        self.NORTH = 1\n",
    "        self.EAST = 2\n",
    "        self.SOUTH = 3\n",
    "        self.action_spaces = [self.WEST, self.NORTH, self.EAST, self.SOUTH]\n",
    "        \n",
    "        # Special states\n",
    "        self.aprime = (0,1)\n",
    "        self.bprime = (0,3)\n",
    "    '''\n",
    "    Agent take an action to move to new state, the environment returns \n",
    "    the reward for the action\n",
    "    '''\n",
    "    def take_action(self, action):\n",
    "        \n",
    "        # Compute the reward and new position of the agent for the action\n",
    "        reward = 0\n",
    "        new_position = self.current_position\n",
    "        if action == self.NORTH:\n",
    "            new_position = (max(self.current_position[0]-1,0), self.current_position[1])\n",
    "            if new_position == self.current_position:\n",
    "                reward = -1\n",
    "        elif action == self.WEST:\n",
    "            new_position = (self.current_position[0], max(self.current_position[1]-1,0))\n",
    "            if new_position == self.current_position:\n",
    "                reward = -1\n",
    "        elif action == self.EAST:\n",
    "            new_position = (self.current_position[0], (self.current_position[1]+1)%5)\n",
    "            if new_position[1] == 0:\n",
    "                reward = -1\n",
    "                new_position = self.current_position\n",
    "        elif action == self.SOUTH:\n",
    "            new_position = ((self.current_position[0]+1)%5, self.current_position[1])\n",
    "            if new_position[0] == 0:\n",
    "                reward = -1\n",
    "                new_position = self.current_position\n",
    "        \n",
    "        # Special reward if the agent is in A prime or B prime, then\n",
    "        # we can ignore the computation of the reward above\n",
    "        if self.current_position == self.aprime:\n",
    "            reward = 10\n",
    "            new_position = (4,1)\n",
    "        elif self.current_position == self.bprime:\n",
    "            reward = 5\n",
    "            new_position = (2,3)\n",
    "            \n",
    "        # Create a new state of the gridworld\n",
    "        new_env = gridworld_env(new_position[0], new_position[1])\n",
    "        \n",
    "        # Return the reward of the action and the new state of the gridworld\n",
    "        return reward, new_env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementation of the Monte Carlo Algorithm Taken from The Book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_monte_carlo_algorithm(episodes, ep_length):\n",
    "    \n",
    "    # The state value of each state\n",
    "    ex_rewards = np.zeros((5,5))\n",
    "    reward_of_state = [[np.array([]) for j in range(0,5)] for i in range(0, 5)]\n",
    "\n",
    "    # Number of steps in each episode\n",
    "    time = ep_length\n",
    "\n",
    "    # Loop for each episode\n",
    "    for episode in range(0, episodes):\n",
    "    \n",
    "        # Create a grid world environment\n",
    "        env = gridworld_env(random.randint(0,4), random.randint(0,4))\n",
    "    \n",
    "        # The sequence of states and reward with respect to time t\n",
    "        states_sequence = [0] * time\n",
    "        reward_sequence = [0] * time\n",
    "    \n",
    "        # Generate a sequence of states and rewards\n",
    "        for t in range(0, time):\n",
    "            # Take a random action\n",
    "            action = random.randint(0,4)\n",
    "            reward, env = env.take_action(action)\n",
    "        \n",
    "            # Store the reward sequence and state sequence\n",
    "            states_sequence[t] = env.current_position\n",
    "            reward_sequence[t] = reward\n",
    "    \n",
    "        # Cumulative reward G\n",
    "        total_reward = 0\n",
    "    \n",
    "        # Loop for each step of episode, t=T−1,T−2,...,0\n",
    "        for t in range(time-1, -1, -1):\n",
    "        \n",
    "            # Get the cumulative reward\n",
    "            total_reward += reward_sequence[t]\n",
    "        \n",
    "            # Check if S_t has appeared in the sequence S_0, S_1, ..., S_t-1 or not\n",
    "            tmp = states_sequence[:t]\n",
    "        \n",
    "            # If the new state hasn't been explored yet:\n",
    "            if states_sequence[t] not in tmp:\n",
    "                returns_st = reward_of_state[states_sequence[t][0]][states_sequence[t][1]]\n",
    "                reward_of_state[states_sequence[t][0]][states_sequence[t][1]] = np.append(returns_st, total_reward)\n",
    "            \n",
    "                ex_rewards[states_sequence[t][0], states_sequence[t][1]] = np.average(returns_st)\n",
    "                \n",
    "    return ex_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/user/anaconda3/lib/python3.6/site-packages/numpy/lib/function_base.py:1110: RuntimeWarning: Mean of empty slice.\n",
      "  avg = a.mean(axis)\n",
      "/Users/user/anaconda3/lib/python3.6/site-packages/numpy/core/_methods.py:80: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 3.80924964,  8.00441173,  4.9778377 ,  5.45754813,  2.06795525],\n",
       "       [ 2.14868135,  3.46962311,  2.93941561,  2.48592095,  1.0788098 ],\n",
       "       [ 0.31546066,  1.02142393,  0.98263021,  1.984307  , -0.30994815],\n",
       "       [-1.27444658, -0.70573769, -0.60383337, -0.95304348, -1.62473246],\n",
       "       [-2.56030312,  2.18911084, -1.88304962, -2.14988013, -2.74986633]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "naive_monte_carlo_algorithm(100000, 50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
