{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import Series, DataFrame\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation of The Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class gridworld_env:\n",
    "    def __init__(self, y,x):\n",
    "        \n",
    "        # The board of the grid world\n",
    "        self.world = np.arange(0,25).reshape((5,5))\n",
    "        \n",
    "        # Current position of the agent on the board\n",
    "        self.current_position = (y,x)\n",
    "        \n",
    "        # Gamma coefficient\n",
    "        self.gamma = 0.9\n",
    "        \n",
    "        # Possible moves\n",
    "        self.WEST = 0\n",
    "        self.NORTH = 1\n",
    "        self.EAST = 2\n",
    "        self.SOUTH = 3\n",
    "        self.action_spaces = [self.WEST, self.NORTH, self.EAST, self.SOUTH]\n",
    "        \n",
    "        # Special states\n",
    "        self.aprime = (0,1)\n",
    "        self.bprime = (0,3)\n",
    "    '''\n",
    "    Agent take an action to move to new state, the environment returns \n",
    "    the reward for the action\n",
    "    '''\n",
    "    def take_action(self, action):\n",
    "        \n",
    "        # Compute the reward and new position of the agent for the action\n",
    "        reward = 0\n",
    "        new_position = self.current_position\n",
    "        if action == self.NORTH:\n",
    "            new_position = (max(self.current_position[0]-1,0), self.current_position[1])\n",
    "            if new_position == self.current_position:\n",
    "                reward = -1\n",
    "        elif action == self.WEST:\n",
    "            new_position = (self.current_position[0], max(self.current_position[1]-1,0))\n",
    "            if new_position == self.current_position:\n",
    "                reward = -1\n",
    "        elif action == self.EAST:\n",
    "            new_position = (self.current_position[0], (self.current_position[1]+1)%5)\n",
    "            if new_position[1] == 0:\n",
    "                reward = -1\n",
    "                new_position = self.current_position\n",
    "        elif action == self.SOUTH:\n",
    "            new_position = ((self.current_position[0]+1)%5, self.current_position[1])\n",
    "            if new_position[0] == 0:\n",
    "                reward = -1\n",
    "                new_position = self.current_position\n",
    "        \n",
    "        # Special reward if the agent is in A prime or B prime, then\n",
    "        # we can ignore the computation of the reward above\n",
    "        if self.current_position == self.aprime:\n",
    "            reward = 10\n",
    "            new_position = (4,1)\n",
    "        elif self.current_position == self.bprime:\n",
    "            reward = 5\n",
    "            new_position = (2,3)\n",
    "            \n",
    "        # Create a new state of the gridworld\n",
    "        new_env = gridworld_env(new_position[0], new_position[1])\n",
    "        \n",
    "        # Return the reward of the action and the new state of the gridworld\n",
    "        return reward, new_env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation of The Tabular TD(0) for Estimating $v_\\pi$\n",
    "\n",
    "Input: the policy $\\pi$ to be evaluated <br>\n",
    "Algorithm parameters: step size $\\alpha \\in (0,1]$, small $\\epsilon>0$ <br>\n",
    "Initialize $V(s)$, for all $s\\in S^+$, arbitrarily except that $V(terminal) = 0$ <br>\n",
    "<br>\n",
    "Loop for each episode: <br>\n",
    "$\\quad$ Initialize $s$ <br>\n",
    "$\\quad$ Loop for each step of episode: <br>\n",
    "$\\quad\\quad$ $a \\leftarrow$ action given by $\\pi$ for state $s$ <br>\n",
    "$\\quad\\quad$ Take action $a$, observe reward $r$ and next state $s'$ <br>\n",
    "$\\quad\\quad$ $V(s) \\leftarrow V(s) + \\alpha\\Big[ R + \\gamma V(s')-V(s) \\Big]$ <br>\n",
    "$\\quad\\quad$ $s \\leftarrow s'$ <br>\n",
    "$\\quad$ Until $s$ is terminal <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def td0_learning(episodes, steps):\n",
    "    \n",
    "    # State value talbe for every states in the grid world environment\n",
    "    s_table = np.zeros((5,5))\n",
    "    \n",
    "    # Learning rate\n",
    "    alpha = 0.001\n",
    "    \n",
    "    # Discount rate\n",
    "    gamma = 0.9\n",
    "    \n",
    "    for episode in range(0, episodes):\n",
    "        \n",
    "        # Initialize new environment with a random starting position\n",
    "        new_x = random.randint(0, 3)\n",
    "        new_y = random.randint(0, 3)\n",
    "        env = gridworld_env(new_y, new_x)\n",
    "        \n",
    "        for step in range(0, steps):\n",
    "            \n",
    "            # Take a random action a\n",
    "            a = random.randint(0, 3)\n",
    "            reward, new_env = env.take_action(a)\n",
    "            \n",
    "            # Get the state of the agent prior to taking action a\n",
    "            prev_y = env.current_position[0]\n",
    "            prev_x = env.current_position[1]\n",
    "            \n",
    "            # Get the state of the agent after taking action a\n",
    "            curr_y = new_env.current_position[0]\n",
    "            curr_x = new_env.current_position[1]\n",
    "            \n",
    "            # Update the value of the state prior to taking action a\n",
    "            s_table[prev_y, prev_x] += alpha * (reward + gamma * s_table[curr_y, curr_x] - s_table[prev_y, prev_x]) \n",
    "            \n",
    "            # Update the new environment\n",
    "            env = new_env\n",
    "            \n",
    "    # Return the approximation of the value of states\n",
    "    return s_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the TD(0) Algorithm with 100,000 Episodes of Length 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 3.3505652   8.79149056  4.49336496  5.32596168  1.49893126]\n",
      " [ 1.52869309  2.98058435  2.26885413  1.98484334  0.53863336]\n",
      " [ 0.06655148  0.73839183  0.68221542  0.39102632 -0.38323434]\n",
      " [-0.98744136 -0.41182561 -0.32071835 -0.58421933 -1.18358308]\n",
      " [-1.83778317 -1.34291497 -1.20144778 -1.43113267 -1.9777177 ]]\n"
     ]
    }
   ],
   "source": [
    "print(td0_learning(100000, 100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
