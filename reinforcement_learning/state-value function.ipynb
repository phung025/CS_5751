{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import Series, DataFrame\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation of The Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class gridworld_env:\n",
    "    def __init__(self, y,x):\n",
    "        \n",
    "        # The board of the grid world\n",
    "        self.world = np.arange(0,25).reshape((5,5))\n",
    "        \n",
    "        # Current position of the agent on the board\n",
    "        self.current_position = (y,x)\n",
    "        \n",
    "        # Gamma coefficient\n",
    "        self.gamma = 0.9\n",
    "        \n",
    "        # Possible moves\n",
    "        self.WEST = 0\n",
    "        self.NORTH = 1\n",
    "        self.EAST = 2\n",
    "        self.SOUTH = 3\n",
    "        self.action_spaces = [self.WEST, self.NORTH, self.EAST, self.SOUTH]\n",
    "        \n",
    "        # Special states\n",
    "        self.aprime = (0,1)\n",
    "        self.bprime = (0,3)\n",
    "    '''\n",
    "    Agent take an action to move to new state, the environment returns \n",
    "    the reward for the action\n",
    "    '''\n",
    "    def take_action(self, action):\n",
    "        \n",
    "        # Compute the reward and new position of the agent for the action\n",
    "        reward = 0\n",
    "        new_position = self.current_position\n",
    "        if action == self.NORTH:\n",
    "            new_position = (max(self.current_position[0]-1,0), self.current_position[1])\n",
    "            if new_position == self.current_position:\n",
    "                reward = -1\n",
    "        elif action == self.WEST:\n",
    "            new_position = (self.current_position[0], max(self.current_position[1]-1,0))\n",
    "            if new_position == self.current_position:\n",
    "                reward = -1\n",
    "        elif action == self.EAST:\n",
    "            new_position = (self.current_position[0], (self.current_position[1]+1)%5)\n",
    "            if new_position[1] == 0:\n",
    "                reward = -1\n",
    "                new_position = self.current_position\n",
    "        elif action == self.SOUTH:\n",
    "            new_position = ((self.current_position[0]+1)%5, self.current_position[1])\n",
    "            if new_position[0] == 0:\n",
    "                reward = -1\n",
    "                new_position = self.current_position\n",
    "        \n",
    "        # Special reward if the agent is in A prime or B prime, then\n",
    "        # we can ignore the computation of the reward above\n",
    "        if self.current_position == self.aprime:\n",
    "            reward = 10\n",
    "            new_position = (4,1)\n",
    "        elif self.current_position == self.bprime:\n",
    "            reward = 5\n",
    "            new_position = (2,3)\n",
    "            \n",
    "        # Create a new state of the gridworld\n",
    "        new_env = gridworld_env(new_position[0], new_position[1])\n",
    "        \n",
    "        # Return the reward of the action and the new state of the gridworld\n",
    "        return reward, new_env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation of The State Value Function\n",
    "Estimate the state value function that estimate how good it is for an agent to be in a given state\n",
    "\n",
    "$$v_\\pi(s) = \\sum_a \\pi(a|s) \\sum_{s', r}P(s',r|s,a)\\Big[\\ r + \\gamma v_\\pi(s') \\Big] \\quad\\forall s\\in S$$\n",
    "\n",
    "s: given state <br>\n",
    "s': future state <br>\n",
    "a: an action the agent can make in that state <br>\n",
    "r: possible reward <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def approx_state_val():\n",
    "    \n",
    "    # Matrix containing the probability of choosing an action\n",
    "    actions_p = np.array([0.25, 0.25, 0.25, 0.25])\n",
    "    \n",
    "    # Possible actions an agent can make\n",
    "    actions = [0, 1, 2, 3]\n",
    "    \n",
    "    # Table displaying the value of the states\n",
    "    # The environment is a 5x5 grid world\n",
    "    state_val_table = np.zeros((5,5))\n",
    "    \n",
    "    # Discount rate\n",
    "    gamma = 0.9\n",
    "    \n",
    "    while True:\n",
    "        \n",
    "        # Temporary state value table for the current run\n",
    "        tmp_table = np.zeros((5,5))\n",
    "        \n",
    "        # For each state in the state space, approximate the value of that state\n",
    "        for x in range(0, 5):\n",
    "            for y in range(0, 5):\n",
    "                \n",
    "                # Initialize the environment\n",
    "                env = gridworld_env(y,x)\n",
    "                \n",
    "                new_val = 0\n",
    "                for action in actions:\n",
    "                    reward, new_env = env.take_action(action)\n",
    "                    \n",
    "                    # Get new position of the agent after taking an action\n",
    "                    new_position = new_env.current_position\n",
    "                    new_x = new_position[1]\n",
    "                    new_y = new_position[0]\n",
    "                    \n",
    "                    # Approximate the new value of the state\n",
    "                    # Since the action is deterministic, the conditional probability\n",
    "                    # in the equation is 1\n",
    "                    new_val += actions_p[action] * (reward + gamma * state_val_table[new_y, new_x])\n",
    "    \n",
    "                # Update the state value table\n",
    "                tmp_table[y, x] = new_val\n",
    "        \n",
    "        # Check if the state value table converge\n",
    "        if abs(np.sum(tmp_table) - np.sum(state_val_table)) < 0.00001:\n",
    "            state_val_table = tmp_table\n",
    "            break\n",
    "        \n",
    "        # If not converge yet, set the temporary table to be the new\n",
    "        # value state table\n",
    "        state_val_table = tmp_table\n",
    "        \n",
    "    return state_val_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approximate the Value of The States"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 3.30899965  8.78929517  4.42762249  5.3223709   1.49218207]\n",
      " [ 1.52159138  2.99232117  2.25014326  1.90757502  0.54740602]\n",
      " [ 0.0508258   0.7381739   0.67311657  0.35818953 -0.40313783]\n",
      " [-0.97358899 -0.43549212 -0.35487896 -0.58560178 -1.18307177]\n",
      " [-1.85769724 -1.34522795 -1.22926395 -1.42291484 -1.97517574]]\n"
     ]
    }
   ],
   "source": [
    "print(approx_state_val())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
