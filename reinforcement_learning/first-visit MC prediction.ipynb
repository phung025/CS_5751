{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import Series, DataFrame\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gridworld Environment Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class gridworld_env:\n",
    "    def __init__(self, x, y):\n",
    "        \n",
    "        # The board of the grid world\n",
    "        self.world = np.arange(0,25).reshape((5,5))\n",
    "        \n",
    "        # Current position of the agent on the board\n",
    "        self.current_position = (x,y)\n",
    "        \n",
    "        # Gamma coefficient\n",
    "        self.gamma = 0.9\n",
    "        \n",
    "        # Possible moves\n",
    "        self.WEST = 0\n",
    "        self.NORTH = 1\n",
    "        self.EAST = 2\n",
    "        self.SOUTH = 3\n",
    "        self.action_spaces = [self.WEST, self.NORTH, self.EAST, self.SOUTH]\n",
    "        \n",
    "        # Special states\n",
    "        self.aprime = (0,1)\n",
    "        self.bprime = (0,3)\n",
    "    '''\n",
    "    Agent take an action to move to new state, the environment returns \n",
    "    the reward for the action\n",
    "    '''\n",
    "    def take_action(self, action):\n",
    "        \n",
    "        # Compute the reward and new position of the agent for the action\n",
    "        reward = 0\n",
    "        new_position = self.current_position\n",
    "        if action == self.NORTH:\n",
    "            new_position = (max(self.current_position[0]-1,0), self.current_position[1])\n",
    "            if new_position == self.current_position:\n",
    "                reward = -1\n",
    "        elif action == self.WEST:\n",
    "            new_position = (self.current_position[0], max(self.current_position[1]-1,0))\n",
    "            if new_position == self.current_position:\n",
    "                reward = -1\n",
    "        elif action == self.EAST:\n",
    "            new_position = (self.current_position[0], (self.current_position[1]+1)%5)\n",
    "            if new_position[1] == 0:\n",
    "                reward = -1\n",
    "                new_position = self.current_position\n",
    "        elif action == self.SOUTH:\n",
    "            new_position = ((self.current_position[0]+1)%5, self.current_position[1])\n",
    "            if new_position[0] == 0:\n",
    "                reward = -1\n",
    "                new_position = self.current_position\n",
    "        \n",
    "        # Special reward if the agent is in A prime or B prime, then\n",
    "        # we can ignore the computation of the reward above\n",
    "        if self.current_position == self.aprime:\n",
    "            reward = 10\n",
    "            new_position = (4,1)\n",
    "        elif self.current_position == self.bprime:\n",
    "            reward = 5\n",
    "            new_position = (2,3)\n",
    "            \n",
    "        # Create a new state of the gridworld\n",
    "        new_env = gridworld_env(new_position[0], new_position[1])\n",
    "        \n",
    "        # Return the reward of the action and the new state of the gridworld\n",
    "        return reward, new_env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation of the Monte Carlo Algorithm Taken from The Book"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input: a policy $\\pi$ to be evaluated\n",
    "\n",
    "Initialize:\n",
    "\n",
    "$\\quad V(s) \\in\\textbf{R}$, arbitarily, for all $s\\in S$\n",
    "\n",
    "$\\quad Returns(s) \\leftarrow$ an empty list, for all $s\\in S$\n",
    "\n",
    "Loop forever (for each episode):\n",
    "\n",
    "$\\quad$Generate an episode following $\\pi: S_0, A_0, R_1, S_1, A_1, R_2, ..., S_{T-1}, A_{T-1}, R_T$\n",
    "\n",
    "$\\quad$ $G \\leftarrow 0$\n",
    "\n",
    "$\\quad$Loop for each step of episode, $T=T-1, T-2, ..., 0$:\n",
    "\n",
    "$\\quad\\quad$ $G \\leftarrow G + R_{t+1}$\n",
    "\n",
    "$\\quad\\quad$ Unless $S_t$ appears in $S_0, S_1, ..., S_{t-1}$:\n",
    "\n",
    "$\\quad\\quad\\quad$ Append $G$ to $Returns(S_t)$\n",
    "\n",
    "$\\quad\\quad\\quad$ $V(S_t) \\leftarrow average(Returns(S_t))$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def first_visit_mc(episodes, ep_length):\n",
    "    \n",
    "    # The state value of each state\n",
    "    ex_rewards = np.zeros((5,5))\n",
    "    reward_of_state = [[np.array([]) for j in range(0,5)] for i in range(0, 5)]\n",
    "\n",
    "    # Number of steps in each episode\n",
    "    time = ep_length\n",
    "\n",
    "    # Loop for each episode\n",
    "    for episode in range(0, episodes):\n",
    "    \n",
    "        # Create a grid world environment\n",
    "        env = gridworld_env(random.randint(0,4), random.randint(0,4))\n",
    "    \n",
    "        # The sequence of states and reward with respect to time t\n",
    "        states_sequence = [0] * time\n",
    "        reward_sequence = [0] * time\n",
    "    \n",
    "        # Generate a sequence of states and rewards\n",
    "        for t in range(0, time):\n",
    "            # Take a random action\n",
    "            action = random.randint(0,4)\n",
    "            reward, env = env.take_action(action)\n",
    "        \n",
    "            # Store the reward sequence and state sequence\n",
    "            states_sequence[t] = env.current_position\n",
    "            reward_sequence[t] = reward\n",
    "    \n",
    "        # Cumulative reward G\n",
    "        total_reward = 0\n",
    "    \n",
    "        # Loop for each step of episode, t=T−1,T−2,...,0\n",
    "        for t in range(time-1, -1, -1):\n",
    "        \n",
    "            # Get the cumulative reward\n",
    "            total_reward += reward_sequence[t]\n",
    "        \n",
    "            # Check if S_t has appeared in the sequence S_0, S_1, ..., S_t-1 or not\n",
    "            tmp = states_sequence[:t]\n",
    "        \n",
    "            # If the new state hasn't been explored yet:\n",
    "            if states_sequence[t] not in tmp:\n",
    "                returns_st = reward_of_state[states_sequence[t][0]][states_sequence[t][1]]\n",
    "                reward_of_state[states_sequence[t][0]][states_sequence[t][1]] = np.append(returns_st, total_reward)\n",
    "            \n",
    "                ex_rewards[states_sequence[t][0], states_sequence[t][1]] = np.average(returns_st)\n",
    "                \n",
    "    return ex_rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute The Value of The States"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/namphung/Programs/Anaconda/lib/python3.6/site-packages/numpy/lib/function_base.py:1128: RuntimeWarning: Mean of empty slice.\n",
      "  avg = a.mean(axis)\n",
      "/home/namphung/Programs/Anaconda/lib/python3.6/site-packages/numpy/core/_methods.py:80: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.42346646  4.49832068  2.43614959  2.44935581 -0.21592883]\n",
      " [-0.87909025  0.09728799 -0.39114339 -0.77355317 -1.94875025]\n",
      " [-3.18777026 -2.69776396 -2.77507775 -1.8598872  -3.82164726]\n",
      " [-5.07718544 -4.66553666 -4.600446   -4.85332853 -5.3965112 ]\n",
      " [-6.41372686 -1.93133931 -5.91791918 -6.08204328 -6.52292549]]\n"
     ]
    }
   ],
   "source": [
    "print(first_visit_mc(100000, 500))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Another Implementation of First-Visit Monte Carlo Prediction Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
